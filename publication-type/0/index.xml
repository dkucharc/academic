<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>0 | DK's page</title><link>https://dkucharc.github.io/academic/publication-type/0/</link><atom:link href="https://dkucharc.github.io/academic/publication-type/0/index.xml" rel="self" type="application/rss+xml"/><description>0</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 05 Dec 2022 00:00:00 +0000</lastBuildDate><image><url>https://dkucharc.github.io/academic/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url><title>0</title><link>https://dkucharc.github.io/academic/publication-type/0/</link></image><item><title>Conformal Prediction and Distribution-Free Uncertainty Quantification</title><link>https://dkucharc.github.io/academic/projects/conformal_prediction/</link><pubDate>Mon, 05 Dec 2022 00:00:00 +0000</pubDate><guid>https://dkucharc.github.io/academic/projects/conformal_prediction/</guid><description>&lt;p>Black-box machine learning models are now routinely used in high-risk settings, like medical diagnostics, which demand uncertainty quantification to avoid consequential model failures. Conformal prediction is a user-friendly paradigm for creating statistically rigorous uncertainty sets/intervals for the predictions of such models. Critically, the sets are valid in a distribution-free sense: they possess explicit, non-asymptotic guarantees even without distributional assumptions or model assumptions. One can use conformal prediction with any pre-trained model, such as a neural network, to produce sets that are guaranteed to contain the ground truth with a user-specified probability, such as 90%. It is easy-to-understand, easy-to-use, and general, applying naturally to problems arising in the fields of computer vision, natural language processing, deep reinforcement learning, and so on.&lt;/p></description></item><item><title>Mathematical aspects of data drift in machine learning</title><link>https://dkucharc.github.io/academic/projects/data_drift/</link><pubDate>Mon, 05 Dec 2022 00:00:00 +0000</pubDate><guid>https://dkucharc.github.io/academic/projects/data_drift/</guid><description>&lt;p>Data drift is a common problem in machine learning where the statistical properties of the data used for model training change over time. This can lead to a drop in model performance, as the model is no longer optimized for the new distribution of data.&lt;/p>
&lt;p>Data drift can occur for many reasons, such as changes in the data collection process, changes in user behavior, or changes in the underlying system being modeled. To mitigate data drift, it is important to monitor the performance of the model over time and retrain it with updated data when necessary.&lt;/p>
&lt;p>There are several techniques that can be used to detect and address data drift, including:&lt;/p>
&lt;p>Monitoring key performance metrics such as accuracy, precision, and recall over time to detect drops in model performance.&lt;/p>
&lt;p>Using statistical tests to compare the distributions of the data used for training and testing the model.&lt;/p>
&lt;p>Using data augmentation techniques such as synthetic data generation to supplement the training data and make the model more robust to changes in the data distribution.&lt;/p>
&lt;p>Employing domain adaptation techniques to adapt the model to new data distributions, while minimizing the need for retraining the entire model.&lt;/p>
&lt;p>By proactively monitoring and addressing data drift, machine learning models can maintain high levels of accuracy and continue to provide valuable insights even as data distribution changes over time.&lt;/p></description></item></channel></rss>