<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Master | DK's page</title><link>https://dkucharc.github.io/academic/tag/master/</link><atom:link href="https://dkucharc.github.io/academic/tag/master/index.xml" rel="self" type="application/rss+xml"/><description>Master</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 21 Mar 2023 00:00:00 +0000</lastBuildDate><image><url>https://dkucharc.github.io/academic/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url><title>Master</title><link>https://dkucharc.github.io/academic/tag/master/</link></image><item><title>Matrix factorization techniques for recommender systems</title><link>https://dkucharc.github.io/academic/projects/20222023_factorization_machines/</link><pubDate>Tue, 21 Mar 2023 00:00:00 +0000</pubDate><guid>https://dkucharc.github.io/academic/projects/20222023_factorization_machines/</guid><description>&lt;p>Matrix factorization is a popular technique in machine learning used for dimensionality reduction, data compression, and collaborative filtering. It involves decomposing a matrix into two or more matrices that represent latent features or factors.&lt;/p>
&lt;p>The goal of matrix factorization is to extract meaningful latent factors from a matrix that can be used to make predictions or recommendations. For example, in collaborative filtering, matrix factorization can be used to predict a user&amp;rsquo;s rating for an item based on their historical ratings and the ratings of similar users.&lt;/p>
&lt;p>Matrix factorization techniques are widely used in recommender systems to provide personalized recommendations to users based on their historical behavior or preferences. Here are some popular matrix factorization techniques used for recommender systems:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Singular Value Decomposition (SVD): SVD is a widely used matrix factorization technique for recommender systems. It decomposes the user-item interaction matrix into two lower-rank matrices, which represent latent factors. These factors represent the user&amp;rsquo;s preferences and the item&amp;rsquo;s characteristics.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Alternating Least Squares (ALS): ALS is another popular matrix factorization technique for recommender systems. It iteratively minimizes the error between the original matrix and the reconstructed matrix using alternating optimization techniques.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Non-negative Matrix Factorization (NMF): NMF is a matrix factorization technique that factorizes the user-item interaction matrix into two non-negative matrices, which represent the user preferences and item characteristics. It is often used for recommender systems that require non-negative values.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Bayesian Personalized Ranking (BPR): BPR is a matrix factorization technique that learns latent factors for users and items by maximizing the likelihood of the observed interactions while minimizing the likelihood of unobserved interactions.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Factorization Machines (FM): FM is a matrix factorization technique that can capture complex interactions between user preferences and item characteristics. It is a more powerful technique than SVD or NMF, but it requires more computational resources.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>These techniques have different strengths and weaknesses and can be used depending on the specific needs of the recommender system.&lt;/p></description></item><item><title>Conformal Prediction and Distribution-Free Uncertainty Quantification</title><link>https://dkucharc.github.io/academic/projects/conformal_prediction/</link><pubDate>Mon, 05 Dec 2022 00:00:00 +0000</pubDate><guid>https://dkucharc.github.io/academic/projects/conformal_prediction/</guid><description>&lt;p>Black-box machine learning models are now routinely used in high-risk settings, like medical diagnostics, which demand uncertainty quantification to avoid consequential model failures. Conformal prediction is a user-friendly paradigm for creating statistically rigorous uncertainty sets/intervals for the predictions of such models. Critically, the sets are valid in a distribution-free sense: they possess explicit, non-asymptotic guarantees even without distributional assumptions or model assumptions. One can use conformal prediction with any pre-trained model, such as a neural network, to produce sets that are guaranteed to contain the ground truth with a user-specified probability, such as 90%. It is easy-to-understand, easy-to-use, and general, applying naturally to problems arising in the fields of computer vision, natural language processing, deep reinforcement learning, and so on.&lt;/p></description></item></channel></rss>