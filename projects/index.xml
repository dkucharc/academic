<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Research Projects | DK's page</title><link>https://dkucharc.github.io/academic/projects/</link><atom:link href="https://dkucharc.github.io/academic/projects/index.xml" rel="self" type="application/rss+xml"/><description>Research Projects</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 21 Mar 2023 00:00:00 +0000</lastBuildDate><image><url>https://dkucharc.github.io/academic/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url><title>Research Projects</title><link>https://dkucharc.github.io/academic/projects/</link></image><item><title>Beyond classical dimensionality reduction techniques</title><link>https://dkucharc.github.io/academic/projects/dimensionality_reduction/</link><pubDate>Tue, 21 Mar 2023 00:00:00 +0000</pubDate><guid>https://dkucharc.github.io/academic/projects/dimensionality_reduction/</guid><description>&lt;p>Dimensionality reduction is a technique used in machine learning to reduce the number of features in a dataset. This can help to improve the performance of machine learning models by reducing the amount of noise and redundancy in the data, as well as making the data more computationally efficient to process. Some of the modern methods of dimensionality reduction include:&lt;/p>
&lt;p>Principal Component Analysis (PCA): PCA is a popular technique used to reduce the dimensionality of a dataset by identifying the principal components that explain the most variance in the data. It works by projecting the data onto a new set of orthogonal axes, with each axis representing a principal component.&lt;/p>
&lt;p>t-Distributed Stochastic Neighbor Embedding (t-SNE): t-SNE is a nonlinear dimensionality reduction technique that is particularly effective for visualizing high-dimensional data. It works by mapping the high-dimensional data points to a low-dimensional space, while preserving the pairwise distances between the data points.&lt;/p>
&lt;p>Autoencoders: Autoencoders are neural networks that can be used for unsupervised learning and dimensionality reduction. They work by compressing the input data into a lower-dimensional representation, and then reconstructing the original input from this compressed representation.&lt;/p>
&lt;p>Non-negative Matrix Factorization (NMF): NMF is a matrix factorization technique that can be used for dimensionality reduction and feature extraction. It works by factorizing a non-negative matrix into two non-negative matrices, where the resulting factors can be interpreted as a lower-dimensional representation of the original data.&lt;/p>
&lt;p>UMAP (Uniform Manifold Approximation and Projection): UMAP is a newer technique for dimensionality reduction that is gaining popularity. It works by constructing a low-dimensional representation of the data that preserves both local and global structure, making it particularly useful for clustering and visualization tasks.&lt;/p>
&lt;p>These modern methods of dimensionality reduction offer a range of options to choose from, and selecting the most appropriate technique for a particular dataset depends on the nature of the data and the specific goals of the analysis.&lt;/p></description></item><item><title>Linear unmixing models on hyperspectral data</title><link>https://dkucharc.github.io/academic/projects/spectral_unmixing/</link><pubDate>Tue, 21 Mar 2023 00:00:00 +0000</pubDate><guid>https://dkucharc.github.io/academic/projects/spectral_unmixing/</guid><description>&lt;p>Linear unmixing is a popular technique used for analyzing hyperspectral data. Hyperspectral imaging is a remote sensing technique that captures the electromagnetic radiation reflected from the Earth&amp;rsquo;s surface across hundreds of narrow spectral bands. Linear unmixing is used to extract information about the different materials present in the scene based on their spectral signatures.&lt;/p>
&lt;p>The linear unmixing model assumes that each pixel in the hyperspectral image can be represented as a linear combination of the spectral signatures of the materials present in that pixel. In other words, the pixel&amp;rsquo;s spectrum is a weighted sum of the spectra of the constituent materials. This model is expressed mathematically as:&lt;/p>
&lt;p>(位) = (位)&lt;/p>
&lt;p>where (位) is the observed spectrum at wavelength 位, (位) is the matrix of spectral signatures for the materials of interest, and  is the abundance matrix that represents the proportion of each material present in the pixel.&lt;/p>
&lt;p>The linear unmixing process involves estimating the abundance matrix  for each pixel in the image. This is typically done using optimization techniques such as least squares, non-negative least squares, or constrained least squares. Once the abundance matrix is estimated, the abundance maps can be generated for each material, providing information about the spatial distribution of the different materials in the scene.&lt;/p>
&lt;p>Linear unmixing is widely used in applications such as mineral mapping, vegetation analysis, and urban land-use classification, among others. It is a powerful tool for extracting information from hyperspectral data and can provide valuable insights into the Earth&amp;rsquo;s surface and its composition.&lt;/p></description></item><item><title>Optimization Methods for Financial Index Tracking</title><link>https://dkucharc.github.io/academic/projects/index_tracking/</link><pubDate>Tue, 21 Mar 2023 00:00:00 +0000</pubDate><guid>https://dkucharc.github.io/academic/projects/index_tracking/</guid><description>&lt;p>Financial index tracking is a popular investment strategy that involves investing in a portfolio of securities that mirrors the composition and performance of a particular financial index, such as the S&amp;amp;P 500 or the Dow Jones Industrial Average. The goal of index tracking is to achieve returns that closely match the performance of the chosen index, which is considered to be a benchmark for the overall market.&lt;/p>
&lt;p>Index tracking is typically achieved through passive investing, which involves buying and holding the constituent securities of the index in proportion to their weights in the index. This approach allows investors to benefit from the growth of the overall market without having to make individual stock selections.&lt;/p>
&lt;p>There are several advantages to index tracking. First, it offers broad exposure to a diversified portfolio of securities, which can reduce the risk of losses due to individual stock performance. Second, it typically has lower fees and expenses compared to actively managed funds, which can improve the overall returns for investors. Finally, it is a simple and transparent investment strategy that is easy to understand and implement.&lt;/p></description></item><item><title>Semantic Segmentation of Remote Sensing Images</title><link>https://dkucharc.github.io/academic/projects/unet_remote_sensing/</link><pubDate>Tue, 21 Mar 2023 00:00:00 +0000</pubDate><guid>https://dkucharc.github.io/academic/projects/unet_remote_sensing/</guid><description>&lt;p>Semantic segmentation in remote sensing refers to the process of segmenting an image obtained from a remote sensing platform (such as a satellite or drone) into different regions or objects based on their semantic meaning. This involves assigning each pixel in the image to a particular class or category, such as vegetation, water, buildings, roads, and so on.&lt;/p>
&lt;p>It has many applications, such as land cover classification, urban planning, environmental monitoring, and disaster management. It can be performed using a variety of techniques, including supervised and unsupervised machine learning algorithms, deep learning models, and image processing techniques.&lt;/p>
&lt;p>Some popular deep learning models used for semantic segmentation in remote sensing include U-Net, FCN, SegNet, and DeepLab. These models use convolutional neural networks (CNNs) to extract features from the input image and then apply a decoder to generate a pixel-wise segmentation map.&lt;/p></description></item><item><title>Time-series forecasting models based on DARTs</title><link>https://dkucharc.github.io/academic/projects/time_series_dart/</link><pubDate>Tue, 21 Mar 2023 00:00:00 +0000</pubDate><guid>https://dkucharc.github.io/academic/projects/time_series_dart/</guid><description/></item><item><title>Conformal Prediction and Distribution-Free Uncertainty Quantification</title><link>https://dkucharc.github.io/academic/projects/conformal_prediction/</link><pubDate>Mon, 05 Dec 2022 00:00:00 +0000</pubDate><guid>https://dkucharc.github.io/academic/projects/conformal_prediction/</guid><description>&lt;p>Black-box machine learning models are now routinely used in high-risk settings, like medical diagnostics, which demand uncertainty quantification to avoid consequential model failures. Conformal prediction is a user-friendly paradigm for creating statistically rigorous uncertainty sets/intervals for the predictions of such models. Critically, the sets are valid in a distribution-free sense: they possess explicit, non-asymptotic guarantees even without distributional assumptions or model assumptions. One can use conformal prediction with any pre-trained model, such as a neural network, to produce sets that are guaranteed to contain the ground truth with a user-specified probability, such as 90%. It is easy-to-understand, easy-to-use, and general, applying naturally to problems arising in the fields of computer vision, natural language processing, deep reinforcement learning, and so on.&lt;/p></description></item><item><title>Mathematical aspects of data drift in machine learning</title><link>https://dkucharc.github.io/academic/projects/data_drift/</link><pubDate>Mon, 05 Dec 2022 00:00:00 +0000</pubDate><guid>https://dkucharc.github.io/academic/projects/data_drift/</guid><description>&lt;p>Data drift is a common problem in machine learning where the statistical properties of the data used for model training change over time. This can lead to a drop in model performance, as the model is no longer optimized for the new distribution of data.&lt;/p>
&lt;p>Data drift can occur for many reasons, such as changes in the data collection process, changes in user behavior, or changes in the underlying system being modeled. To mitigate data drift, it is important to monitor the performance of the model over time and retrain it with updated data when necessary.&lt;/p>
&lt;p>There are several techniques that can be used to detect and address data drift, including:&lt;/p>
&lt;p>Monitoring key performance metrics such as accuracy, precision, and recall over time to detect drops in model performance.&lt;/p>
&lt;p>Using statistical tests to compare the distributions of the data used for training and testing the model.&lt;/p>
&lt;p>Using data augmentation techniques such as synthetic data generation to supplement the training data and make the model more robust to changes in the data distribution.&lt;/p>
&lt;p>Employing domain adaptation techniques to adapt the model to new data distributions, while minimizing the need for retraining the entire model.&lt;/p>
&lt;p>By proactively monitoring and addressing data drift, machine learning models can maintain high levels of accuracy and continue to provide valuable insights even as data distribution changes over time.&lt;/p></description></item></channel></rss>